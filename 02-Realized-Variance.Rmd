---
title: "Realized Higher Moment Measures"
description: |
  Part 2: Exploring Realized Higher Moment Measures on the S&P/BMV IPC Index
author:
  - name: Alexis Solis Cancino
    affiliation: ITAM
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
    toc_depth: 2
    css: "resources/theme.css"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  
  # Places figures on their own pages
  fig.pos = 'p',
  
  # Figure resolution and size
  out.width = '100%',
  dpi = 320,
  
  # Latex figure environment
  fig.env = "figure"
)


# --- Load the libraries! ---

# Data Wrangling
library(tidyverse)
library(lubridate)
library(readxl)
library(skimr)
library(glue)

# Time Series
library(timetk)
library(tidyquant)
# library(hms)
# library(modeltime)
# library(tidymodels)

# Visualization
library(scales)
library(patchwork)
library(progress)

# Reproducibility
library(usethis)
library(here)


# Speed
library(dtplyr)
library(arrow)
library(furrr)


# Setup parallelization plan
future::plan(strategy = 'multisession', workers = 11)

# --- Setup the ggplot theme ---
source("resources/ggplot2_themes.R")
theme_set(theme_roboto())


# --- Import Aux Functions ---
source("R/rv_sparse.R")
source("R/split_intraday.R")
```

```{r}
# Read the data
ipc_intraday <- read_parquet(here("data/processed/intraday_ipc.parquet"))
```

# 1. Realized Volatility

To estimate daily volatility of equity returns, we can use the intraday data, in particular, the intraday returns. Since the expected value of the intraday returns is zero, a good estimator of variance is the sum of the squared returns.

We can now compute the realized variance for each day:

```{r}
rv_df <- ipc_intraday %>% 
  filter(!is.na(log_ret)) %>% 
  mutate(log_ret_sq = log_ret^2) %>% 
  group_by(index_date) %>% 
  summarise(RV = sum(log_ret_sq),
            
            # annualized realized volatility
            RVol = sqrt(252 * RV),
            .groups = "drop") %>% 
  
  # Get rid of date "2001-09-12", which has a 0 RV.
  filter(!RVol == 0)

rv_df
```

Now let's plot the RVs:

```{r}
#| layout: "l-body-outset"
#| fig.width: 8
#| fig.asp: 0.618

rv_plot <- rv_df %>% 
  # filter_time("1996" ~ "end") %>%
  ggplot(mapping = aes(x = index_date, y = RVol)) +
  geom_col(color = blog_colors[1]) +
  labs(
    title = "Realized Variance: S&P/BMV IPC Index",
    subtitle = "– Daily Annualized Realized Volatility",
    x = NULL,
    y = NULL,
    caption = "Figure 1: Daily realized volatility (RV) for the S&P/BMV IPC Index.\nThe daily RV is calculated as the sum of squared intraday returns per day and it is then annualized."
  ) +
  scale_x_date(
    date_labels = "%Y", 
    breaks = scales::breaks_pretty(n = 12),
    expand = expansion(c(0.01, 0.01))
  ) +
  scale_y_continuous(
    breaks = scales::breaks_extended(n = 8),
    labels = scales::percent_format(accuracy = .1),
    expand = expansion(c(0, 0.13))
  ) +
  theme(
    panel.grid.major.x = element_blank()
  )

rv_plot
```

Let's check the ACF on the RVs:

```{r}
#| layout: "l-body-outset"
#| fig.width: 8
#| fig.asp: 0.618


acf_plot1 <- rv_df %>% 
  pull(RVol) %>% 
  acf(lag.max = 100, plot = F) %>% 
  broom::tidy() %>% 
  filter(lag != 0) %>% 
  ggplot(aes(lag, acf)) +
  geom_line(aes(color = lag > 10), linewidth = 0.7) +
  
  # Bartlett Errors
  geom_hline(yintercept = 2/sqrt(nrow(rv_df)), 
             linetype = 2,
             color = blog_colors[4]) +
  geom_hline(yintercept = -2/sqrt(nrow(rv_df)), 
             linetype = 2,
             color = blog_colors[4]) +
  
  
  geom_hline(yintercept = 0, linetype = 1) +
  scale_x_continuous(
    breaks = scales::breaks_pretty(n = 12),
    expand = expansion(c(0.015, 0.03))
  ) +
  scale_y_continuous(
    breaks = scales::breaks_extended(n = 8),
    labels = scales::number_format(accuracy = .01),
    expand = expansion(c(0.02, 0.05)),
  ) +
  scale_color_manual(values = blog_colors) +
  guides(color = 'none') +
  labs(
    title = "Auto-Correlation Function for Realized Variance",
    subtitle = "– The x-axis indicates the k-th lag order.",
    x = NULL,
    y = NULL,
    caption = "Figure 2: the blue line indicates the first 10-lag order autocorrelations for the RVs.\nBartlett confidence intervals in dashed, purple lines."
  ) +
  theme(
    panel.grid.major.x = element_blank(),
  )
  
acf_plot1
```

And now let's check the ACF of the daily squared returns:

```{r}
# Get minimum date
min_date <- rv_df %>% 
  pull(index_date) %>% 
  min()

xl_path <- "data/raw/2022_06_03-bloomberg-vimex-ipc-ipcvix.xlsx"

ipc <- read_excel(xl_path, sheet = "IPC") %>% 
  mutate(date = as_date(date))

ipc_daily_clean <- ipc %>% 
  arrange(date) %>% 
  filter(date >= min_date)

ipc_daily_returns <- ipc_daily_clean %>% 
  mutate(
    log_ret = log(close/ lag(close)),
    sq_ret = log_ret^2
    ) %>% 
  drop_na(log_ret)

acf_plot2 <- ipc_daily_returns %>% 
  pull(sq_ret) %>% 
  acf(lag.max = 200, plot = F) %>% 
  broom::tidy() %>% 
  filter(lag != 0) %>% 
  ggplot(aes(lag, acf)) +
  geom_line(aes(color = lag >= 10), size = 1.1) +
  
  # Bartlett Errors
  geom_hline(yintercept = 2/sqrt(nrow(rv_df)), 
             linetype = 2,
             color = blog_colors[4]) +
  geom_hline(yintercept = -2/sqrt(nrow(rv_df)), 
             linetype = 2,
             color = blog_colors[4]) +
  
  
  geom_hline(yintercept = 0, linetype = 1) +
  scale_x_continuous(
    breaks = scales::breaks_pretty(n = 12),
    expand = expansion(c(0.015, 0.03))
  ) +
  scale_y_continuous(
    breaks = scales::breaks_extended(n = 8),
    labels = scales::number_format(accuracy = .01),
    expand = expansion(c(0.02, 0.13)),
    limits = c(-0.1,0.5)
  ) +
  scale_color_manual(values = blog_colors) +
  guides(color = 'none') +
  labs(
    title = "ACF of daily squared returns",
    subtitle = "– The x-axis indicates the k-th lag order.",
    x = NULL,
    y = NULL,
    caption = "Figure 3: the blue line indicates the first 10-lag order autocorrelations for the \nsquared log-returns of the IPC Index. Bartlett confidence intervals in dashed, purple lines."
  ) +
  theme(
    panel.grid.major.x = element_blank(),
  )
```

```{r}
acf_plot1/acf_plot2
```

```{r}
rv_hist1 <- rv_df %>% 
  ggplot(aes(x = RV)) +
  geom_histogram(
    aes(y = after_stat(count)/sum(after_stat(count))),
    bins = 100,
    size = 1.1,
    fill = blog_colors[2],
    color = blog_colors[1]) +
  scale_y_continuous(breaks = breaks_pretty(),
                     labels = scales::percent_format()) +
  scale_x_continuous(breaks = breaks_pretty(8),
                     labels = number_format(scale = 100),
                     expand = expansion(c(0.001, 0.02))) +
  labs(
    title = 'Realized variance',
    x = NULL,
    y = NULL
  ) +
  # theme_fira() +
  theme(panel.grid.major.x = element_blank(),
        plot.background = element_blank())


rv_hist2 <- rv_df %>% 
  mutate(log_rv = log(RV)) %>% 
  ggplot(aes(x = log_rv)) +
  geom_histogram(
    aes(y = after_stat(count)/sum(after_stat(count))),
    bins = 50,
    size = 1.1,
    fill = blog_colors[3],
    color = blog_colors[1]) +
  scale_y_continuous(breaks = breaks_pretty(),
                     labels = scales::percent_format()) +
  scale_x_continuous(breaks = breaks_pretty(8),
                     expand = expansion(c(0.015, 0.02))) +
  labs(
    title = 'Log of realized variance',
    x = NULL,
    y = NULL
  ) +
  # theme_fira() +
  theme(panel.grid.major.x = element_blank(),
        plot.background = element_blank())

rv_hist1/rv_hist2
```

# 2. RV Sparse

An *RV Sparse* estimator consists of calculating the realized variance but with a sample that is less frequent than the 1-minute grid. Instead, it is sampled in an *s*-minute grid (where $s \geq 1$).

Here, we will use different combinations of $s$ such as `s = 5`, `s = 10`, `s = 15`, and `s = 30`. In order to plot a *signature plot*, we will also sample from $s \in [1, 120]$.

## Volatility signature plot

The Sparse RV Estimator is calculated using an *s*-minute grid ($s \geq 1$):

$$ 
RV_{t+1}^s = \sum \limits_{j=1}^{m/s} R^2_{t+js/m} 
$$

Now the problem lies in the choice of the sub-sampling parameter $s$, a problem which represents a typical bias-variance trade-off. The larger the s the less likely we are to get a biased estimate of volatility, but the larger the s the fewer observations we are using and so the more noisy our estimate will be. 

For highly liquid assets, we should choose an $s$ close to 1 and a much larger $s$ for illiquid assets.If the estimated RVs are biased due to liquidity effects while employing a high sample frequency, the bias should go away when the sampling frequency is reduced, or when s is raised.

This is where *volatility signature plots* (VSP) will help; these are a graphical tool that helps us choose the sub-sampling parameter $s$. We do a VSP in the following manner:

1.  Compute $RV_{t+1}^s$ for values of $s$ going from 1 to 120 minutes.

```{r, eval=FALSE}
# Define progress bar
pb <- progress::progress_bar$new(
  format = " computing RVs [:bar] :percent eta: :eta",
  total = 100,
  clear = FALSE
)

# Define a vector with all the "s-grid" parameters
sparse_grid <- seq(from = 0, 
                   to = 65, 
                   by = 5)

sparse_grid[1] <- 1


# Create a tibble with all the nested data
ipc_sparse <- sparse_grid %>% 
  future_map_dfr(~ split_intraday(data = ipc_intraday, 
                                  grid = .x) %>% 
                   rv_sparse(),
                 
                 .progress = T)
rv_sparse
```

```{r unnest-RV-sparse, eval=FALSE}
# Get the mean for each RV sparse:
RV_means <- ipc_sparse %>% 
  group_by(sparse_grid) %>% 
  summarise(
    mean_RV_sparse = mean(RVol_sparse),
    .groups = "drop"
  )
```


```{r get_signature_plot}
library(RcppRoll)

# --- Create function for getting the sparse log returns ---
mutate_sparse_return <- function(.data, 
                                 .return_col = log_ret, 
                                 .trade_id_col = trade_id, 
                                 .sparse) {
  .data %>% 
    group_by(index_date) %>% 
    transmute('log_ret_{.sparse}m':= if_else(trade_id %% .sparse  == 0, 
                                             RcppRoll::roll_sumr(x = {{.return_col}}, n = .sparse,  fill = NA), 
                                             NA_real_)) %>% 
    ungroup() %>% 
    select(-index_date)
}


# --- Prepare data for wrangling ---
indexed_ipc <- ipc_intraday %>% 
  # filter(index_date <= mdy('12/31/1996')) %>%
  select(index_date, index_time, log_ret, last_price) %>% 
  group_by(index_date) %>% 
  mutate(trade_id = row_number() - 1) %>% 
  ungroup()


# --- Define sparse grid and create corresponding returns ---
sparse_grid <- seq(5, 65, by = 5)

ipc_sparse <- indexed_ipc %>% 
  bind_cols(
    map_dfc(.x = sparse_grid,
            .f = ~ mutate_sparse_return(.data = indexed_ipc,
                                        .sparse = .x))
    ) %>% 
  # Also create squared sparse returns
  mutate(
    across(
      .cols = contains("log_ret"),
      .fns  = ~ .x^2,
      .names = "{.col}_squared")
  )


# --- Compute the realized variance and get it annualized ---
rv_sparse_df <- ipc_sparse %>% 
  group_by(index_date) %>% 
  summarise(
    across(
      .cols = contains("squared"),
      .fns = ~ sqrt(sum(.x, na.rm = TRUE) * 252)
    ),
    .groups = 'drop'
  )


# --- Get the average RV per grid parameter ---
rv_means <- rv_sparse_df %>% 
  summarise(
    across(
      .cols = -index_date,
      .fns = mean
    )
  ) %>% 
  mutate(var = "rv") %>% 
  pivot_longer(cols = -var, values_to = 'mean_rv') %>% 
  mutate(
    sparse_grid = str_extract(name, "\\d+"),
    sparse_grid = if_else(is.na(sparse_grid), "1", sparse_grid),
    sparse_grid = as.numeric(sparse_grid)
  )
```


2.  Scatter plot the average RV across days on the vertical axis against $s$ on the horizontal axis.

```{r}
#| layout: "l-body-outset"
#| fig.width: 8
#| fig.asp: 0.618

rv_means %>% 
  ggplot(aes(sparse_grid , mean_rv)) +
  geom_line(colour = blog_colors[6], 
             size = 1.1) +
  geom_point(colour = blog_colors[1], 
             size = 3) +
  labs(
    title = "Signature Plot for the Annualized RV Sparse",
    x = "Minutes",
    y = "RV"
  ) +
  scale_y_continuous(labels = number_format()) +
  scale_x_continuous(breaks = breaks_pretty()) +
  theme(
    panel.grid.major.x = element_blank(),
    panel.background = element_rect(fill = "#F4F3EE", colour = "#F4F3EE"),
    plot.background = element_rect(fill = "#F4F3EE", colour = "#F4F3EE")
  )
```

3.  Look for the smallest $s$ such that the average RV does not change much for values of $s$ larger than this number.

# VIMEX & S&P IPC VIX Indexes

For the volatility index we take a combination between the VIMEX (BMV first started publishing it in XXX, but removed it on XXX) and the S&P IPC VIX ().

However, to justify the usage of both indices as just one, we need to check that their correlation is high.

We start by reading the data:

```{r}
ipc_vix <- read_excel(xl_path, sheet = "IPC-VIX") %>% 
  mutate(date = as_date(date))

ipc_vimex <- read_excel(xl_path, sheet = "VIMEX") %>% 
  mutate(date = as_date(date))
```

We can then plot both indices:

```{r}
consolidated <- ipc %>% 
  mutate(security = "IPC") %>% 
  bind_rows(ipc_vix %>% mutate(security = "IPC-VIX")) %>% 
  bind_rows(ipc_vimex %>% mutate(security = "VIMEX"))

consolidated %>% 
  filter(security != "IPC") %>% 
  ggplot() +
  geom_line(aes(date, close, color = security)) +
  geom_rect(
    data = tibble(),
    aes(xmin = ymd('2015-01-01'), 
        xmax = ymd('2017-07-01'),
        ymin = 0,
        ymax = 70),
    fill = "lightgray",
    alpha = 0.6
    ) +
  scale_y_continuous(breaks = scales::breaks_pretty(n = 8),
                     expand = expansion(c(0.1, 0.06))) +
  scale_x_date(breaks = scales::breaks_pretty(n = 8)) +
  scale_color_manual(values = blog_colors) +
  labs(
    title = "IPC VIX & VIMEX, daily close",
    x     = NULL,
    y     = NULL,
    color = NULL
  ) +
  theme_roboto() +
  theme(panel.grid.major.x = element_blank(),
        legend.box.spacing = unit(0.02, "mm"),
        legend.justification = c(0,0))

initial_vix_date <- ipc_vix %>% slice(1) %>% pull(date)
last_vimex_date <- ipc_vimex %>% slice_tail(n = 1) %>% pull(date)
```

```{r}
consolidated %>% 
  filter(security != "IPC",
         between(date, 
                 initial_vix_date, 
                 last_vimex_date)) %>% 
  ggplot() +
  geom_line(aes(date, close, color = security), 
            size = 1.1) +
  scale_y_continuous(
    breaks = scales::breaks_pretty(n = 5),
    expand = expansion(c(0.2, 0.2))
    ) +
  scale_x_date(
    date_labels = "%Y",
    breaks = timetk::tk_make_timeseries(initial_vix_date, 
                                        last_vimex_date,
                                        by = "year"),
    expand = expansion(c(0, 0.05))
               ) +
  scale_color_manual(values = blog_colors) +
  labs(
    title = "IPC VIX & VIMEX, Daily Close",
    subtitle = glue("Data from {initial_vix_date} to {last_vimex_date}"),
    x     = NULL,
    y     = NULL,
    color = NULL
  ) +
  theme_roboto() +
  theme(panel.grid.major.x = element_blank(),
        legend.box.spacing = unit(0.02, "mm"),
        legend.justification = c(0,0))
```

It seems that they are quite correlated. We can check the degree of linear dependence by calculation the Pearson's correlation coefficient:

```{r}
consolidated %>% 
  filter(security != "IPC",
         between(date, initial_vix_date, last_vimex_date)) %>% 
  select(date, close, security) %>% 
  pivot_wider(
    names_from = security,
    values_from = close
  ) %>% 
  select(-date) %>% 
  corrr::correlate()
```

We obtain 0.95, and so we can conclude that we can use both indices as one. We plot that index next:

```{r}
vimex_component <- ipc_vimex %>% 
  filter(date < initial_vix_date) %>% 
  mutate(security = 'VIMEX')

vol_index <- vimex_component %>%
  bind_rows(ipc_vix %>% mutate(security = "IPC VIX"))

vol_index %>% 
  mutate(security = as_factor(security)) %>% 
  ggplot() +
  geom_line(
    aes(date, close, color = security), 
    size = 1.1,
    ) +
  scale_y_continuous(
    breaks = scales::breaks_pretty(n = 5),
    expand = expansion(c(0.1, 0.2))
    ) +
  scale_x_date(
    date_labels = "%Y",
    breaks = scales::breaks_pretty(n = 8),
    expand = expansion(c(0, 0.05))
  ) +
  scale_color_manual(values = blog_colors) +
  labs(
    title = "IPC Volatility Index, Daily Close",
    subtitle = "— Built from VIMEX and IPC VIX indexes.",
    x     = NULL,
    y     = NULL,
    color = NULL
  ) +
  theme_roboto() +
  theme(panel.grid.major.x = element_blank(),
        legend.box.spacing = unit(0.02, "mm"),
        legend.justification = c(0,0))
```

# HAR Model (see page 100 Christoffersen)

Also check: <https://www.sciencedirect.com/science/article/abs/pii/S0378426621002417> (A Practical Guide to Harnessing the HAR Volatility Model (2021))

The HAR (Heterogeneous Autoregression) model or mixed-frequency model provides a tool for modeling the seemingly long-memory features of realized volatility.

We can start by defining the *h*-day RV from the 1-day RV as follows:

$$
RV_{t-h+1\ :\ t} = \big[RV_{t-h+1} + RV_{t-h+2} + \ \cdots \  +RV_t\big]/h
$$ 
which can be interpreted as the average total variance starting with day $t-h+1$ and through day $t$. Hence, it is reasonable to compute a daily, weekly, and monthly RV defined by this simple moving average:

\begin{align}
RV_{D,t} &= RV_t \\
RV_{W,t} &= [RV_{t-4} + RV_{t-3} + RV_{t-2} + RV_{t-1} + RV_t\big]/5 \\
RV_{M,t} &= [RV_{t-20} + RV_{t-19} + \cdots + RV_{t-1} + RV_t\big]/21
\end{align}



We now forecast a variance for the next month using the HAR model. We use the same rule of thumb employed for estimating GARCH models with daily data: using the past 1,000 observations (which contains a good amount of data but reduces the risk of forecasting through a structural break). Additionally, we compute the model in logs, give the stylized fact that RV follows a log-normal distribution (so that the error term is Normally distributed by construct). The longer-horizon forecasting for a HAR model follows the next equations:

$$
\ln(RV_{t+1:t+k}) = \phi_{0,k} + \phi_{D,k} \ln(RV_{D,t}) + \phi_{W,k} \ln(RV_{W,t}) + \phi_{M,k} \ln(RV_{M,t}) + \varepsilon_{t+1:t+k} \ ,
$$
where:
$$
RV_{t+1:t+k} = \dfrac{RV_{t+1} + RV_{t+2} + \ \cdots \  +RV_{t+k}}{k} 
$$


$$
\sigma^2_{t+1} = \alpha + \beta_1RV_{t, d} + \beta_2RV_{t, w} +\beta_3RV_{t, m} + \gamma VIX^2_t/252 + u_i
$$

```{r data-prep}
# This DataFrame will surely change to the 5-min RV Sparse
rv_for_modeling <- rv_df %>% 
  
  # filter for data when IPC VIX is available
  filter(index_date >= initial_vix_date) %>% 
  select(index_date, RV)

# rv_ts
har_data <- rv_for_modeling %>% 
  mutate(
    rv_target       = RcppRoll::roll_meanl(lead(RV), n = 21),
    rv_daily        = RV,
    rv_weekly       = RcppRoll::roll_meanr(rv_daily, n = 5),
    rv_monthly      = RcppRoll::roll_meanr(rv_daily, n = 21),
    log_rv_target   = log(rv_target),
    log_rv_daily    = log(rv_daily),
    log_rv_weekly   = log(rv_weekly),
    log_rv_monthly  = log(rv_monthly),
    
    # Time-related variables
    year   = year(index_date),
    month  = month(index_date),
    day_id = row_number()
  ) %>% 
  select(index_date, year, month, day_id, contains("log")) %>% 
  left_join(
    vol_index %>% 
      select(date, close) %>% 
      rename('index_date' = date,
             'vix'        = close),
    by = 'index_date'
  ) %>% 
  # impute VIX data
  fill(vix, .direction = "updown") %>% 
  drop_na()

har_data
```

## Question for Aurelio: How to impute the VIX data?

```{r}
library(tidymodels)
library(modeltime)
# https://www.business-science.io/code-tools/2020/06/29/introducing-modeltime.html

# Using rsample
vol_split <- vol_ts %>%
  rolling_origin(initial = nrow(vol_ts) - 21,
                 assess = 21,
                 cumulative = F)


# Using timetk
vol_resamples <- vol_ts %>% 
  time_series_cv(date_var   = index_date,
                 initial    = 12 * 21,
                 assess     = 21,
                 cumulative = FALSE)


vol_resamples <- vol_ts %>% 
  sliding_period(
    index = index_date,
    period = "month",
    lookback = 12,
    assess_stop = 1
  )


vol_rs_df <- vol_resamples %>% 
  slice(1:3) %>% 
  
  # Convert the resamples object into a DataFrame
  tk_time_series_cv_plan()
  
# Plot TS CV Plan
vol_rs_df %>% 
  plot_time_series_cv_plan(.date_var    = index_date,
                           .value       = rv_target,
                           .interactive = FALSE)

```

```{r}
vol_rec <- vol_rs_df %>% 
  recipe(rv_target ~ rv_daily + rv_weekly + rv_monthly + vix) %>% 
  step_mutate(vix = vix^2/252)

```

```{r}
# Pick a model
reg_spec <- linear_reg(penalty = tune(),
                       mixture = tune()) %>% 
  
  # Set mode
  set_mode("regression") %>% 
  
  # Choose engine
  set_engine("glm")

reg_spec
```

```{r}
vol_wf <- workflow() %>% 
  add_recipe(vol_rec) %>% 
  add_model(reg_spec)
```

```{r}
set.seed(123)
tictoc::tic()
vol_tuned <- vol_wf %>% 
  tune_grid(vol_resamples, grid = 5)
tictoc::toc()
```

```{r}
vol_tuned %>% 
  collect_metrics()

vol_tuned %>% 
  show_best(metric = "rmse")
```

We estimate the model with a 1-year rolling window.

## Predictability

We now entertain the following model:


```{tex}
\begin{align}
R_{IPC} $= \alpha + \beta \ \text{VRP} + u_i =\\
        $=\alpha + \beta \ \big(RV - VIX\big) + u_i
\end{align}
```
